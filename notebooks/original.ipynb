{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv(\"us.env\"))  # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "sys.path.append(Path(\"..\").resolve().as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "openai_api_version = \"2023-07-01-preview\"\n",
    "\n",
    "gpt4 = AzureOpenAI(\n",
    "    azure_deployment=\"gpt-4-1106\",\n",
    "    openai_api_version=openai_api_version,\n",
    ")\n",
    "\n",
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    deployment_name=\"text-embedding-ada-002\",\n",
    "    api_version=openai_api_version,\n",
    "    embed_batch_size=100,\n",
    ")\n",
    "\n",
    "Settings.llm = gpt4\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load query engine\n",
    "\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "persist_path = Path(\"index_base_storage\").resolve()\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=persist_path.as_posix())\n",
    "index = load_index_from_storage(storage_context)\n",
    "\n",
    "\n",
    "# configure retriever\n",
    "retriever = index.as_retriever(\n",
    "    similarity_top_k=10,\n",
    ")\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    ")\n",
    "\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "\n",
    "new_summary_tmpl_str = (\n",
    "    \"\"\"As an enthusiastic EdgeDB expert keen to assist, respond to queries in markdown, referencing the given EdgeDB sections and previous history context.\n",
    "    If a \"Learn more\" link appears in the context, verify if it starts with \"https://www.edgedb.com/\".\n",
    "    If so, append it to the answer, otherwise exclude it.\n",
    "    Ensure to utilize the \"new syntax\" in any ```sdl blocks within the answer, replacing old syntax.\n",
    "    The new syntax uses \"->\" over \":\", and omits \"property\" and \"link\" for non-computed properties/links. See below:\n",
    "\n",
    "    Old:\n",
    "    ```sdl\n",
    "    type Movie {{\n",
    "    required property title -> str;\n",
    "    multi link actors -> Person;\n",
    "    }}\n",
    "    ```\n",
    "    New:\n",
    "    ``sdl\n",
    "    type Movie {{\n",
    "    required title: str;\n",
    "    multi actors: Person;\n",
    "    }}\n",
    "    ```\n",
    "    If unable to help based on documentation, respond with: \"Sorry, I don't know how to help with that.\"\n",
    "    EdgeDB sections: ***\n",
    "    {context_str}\n",
    "    ***\n",
    "\n",
    "    Question: ***\n",
    "    {query_str}\n",
    "    ***\n",
    "\n",
    "    Answer in markdown (including related code snippets if available).\n",
    "\n",
    "    After the answer, add a short summary of the question that can be used as a chat title.\n",
    "    Prefix it with: \"????\". The summary should be no more than 35 characters.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "new_summary_tmpl = PromptTemplate(new_summary_tmpl_str)\n",
    "\n",
    "query_engine.update_prompts({\"response_synthesizer:text_qa_template\": new_summary_tmpl})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_query_path = Path(\"benchmarks/tutorial_benchmark.txt\").resolve()\n",
    "# hard_query_path = Path(\"benchmarks/user_benchmark.txt\").resolve()\n",
    "hard_query_path = Path(\"benchmarks/new_hard.txt\").resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import Markdown\n",
    "\n",
    "\n",
    "# define prompt viewing function\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    for k, p in prompts_dict.items():\n",
    "        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
    "        display(Markdown(text_md))\n",
    "        print(p.get_template())\n",
    "        display(Markdown(\"<br><br>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:text_qa_template<br>**Text:** <br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an enthusiastic EdgeDB expert keen to assist, respond to queries in markdown, referencing the given EdgeDB sections and previous history context.\n",
      "    If a \"Learn more\" link appears in the context, verify if it starts with \"https://www.edgedb.com/\".\n",
      "    If so, append it to the answer, otherwise exclude it.\n",
      "    Ensure to utilize the \"new syntax\" in any ```sdl blocks within the answer, replacing old syntax.\n",
      "    The new syntax uses \"->\" over \":\", and omits \"property\" and \"link\" for non-computed properties/links. See below:\n",
      "\n",
      "    Old:\n",
      "    ```sdl\n",
      "    type Movie {{\n",
      "    required property title -> str;\n",
      "    multi link actors -> Person;\n",
      "    }}\n",
      "    ```\n",
      "    New:\n",
      "    ``sdl\n",
      "    type Movie {{\n",
      "    required title: str;\n",
      "    multi actors: Person;\n",
      "    }}\n",
      "    ```\n",
      "    If unable to help based on documentation, respond with: \"Sorry, I don't know how to help with that.\"\n",
      "    EdgeDB sections: ***\n",
      "    {context_str}\n",
      "    ***\n",
      "\n",
      "    Question: ***\n",
      "    {query_str}\n",
      "    ***\n",
      "\n",
      "    Answer in markdown (including related code snippets if available).\n",
      "\n",
      "    After the answer, add a short summary of the question that can be used as a chat title.\n",
      "    Prefix it with: \"????\". The summary should be no more than 35 characters.\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:refine_template<br>**Text:** <br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original query is as follows: {query_str}\n",
      "We have provided an existing answer: {existing_answer}\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "{context_msg}\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompts_dict = query_engine.get_prompts()\n",
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with easy_query_path.open(\"r\") as f:\n",
    "    easy_queries = [query.strip() for query in f.readlines()]\n",
    "\n",
    "with hard_query_path.open(\"r\") as f:\n",
    "    hard_queries = f.read().split(\">>>SEPARATOR<<<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval import PydanticResponse, run_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 183/183 [1:13:35<00:00, 24.13s/it]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "responses = run_queries(query_engine, hard_queries, Path(f\"eval_old_hard_{timestamp}.jsonl\").resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
