{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build EdgeDB RAG\n",
    "\n",
    "## Overview\n",
    "\n",
    "To create a question answering chatbot, we start by **building the index**. \n",
    "\n",
    "Index is made of two components:\n",
    "\n",
    "- vectorstore to store document embeddings and perform vector search.\n",
    "- docstore to store full documents.\n",
    "\n",
    "To build an index, we need to have the documents stored as Markdown files, as well as the metadata JSON file.\n",
    "\n",
    "\n",
    "The process of responding to a user message includes the following stages:\n",
    "\n",
    "1. **Contextualization**: Turning a message into a search query using chat history.\n",
    "2. **Query analysis**: Breaking down the search query into a similarity search part and a filter.\n",
    "3. **Retrieval**: Using vectorstore to retrieve relevant documents with similarity search.\n",
    "4. **Generation**: Producing the final answer based on documents and chat history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "_ = load_dotenv(find_dotenv())  # read local .env file\n",
    "sys.path.append(Path(\"..\").resolve().as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "\n",
    "from src.core.index import Index, ID_KEY\n",
    "from src.core.retriever import build_retriever\n",
    "from src.core.generator import build_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building an index of the documentation goes like this:\n",
    "\n",
    "1. Read the metadata file. It stores information about the docs as a JSON lines file with the following format:\n",
    "\n",
    "    ```json\n",
    "    {\"url\":\"relative/path/to/doc.md\",\"category\":\"edgedb_general\"}\n",
    "    ```\n",
    "\n",
    "    The category is going to be used by the retriever down the line to filter out irrelevant documents.\n",
    "    For example, changelogs and integrations in different languages that the user didn't ask about.\n",
    "\n",
    "2. Based on the metadata file, load the documents and create their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the index or load it from disk\n",
    "\n",
    "persist_path = Path(\"index_storage\").resolve()\n",
    "embedding_function = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-ada-002\", api_version=\"2023-07-01-preview\"\n",
    ")\n",
    "\n",
    "if persist_path.exists():\n",
    "    index = Index.from_persist_path(\n",
    "        persist_path=persist_path,\n",
    "        embedding_function=embedding_function,\n",
    "    )\n",
    "else:\n",
    "    # build from scratch using a metadata file\n",
    "    index = Index.from_metadata(\n",
    "        metadata_path=Path(\"../resources/doc_metadata.jsonl\").resolve(),\n",
    "        lib_path=Path(\"../../docs_md\").resolve(),\n",
    "        persist_path=persist_path,\n",
    "        embedding_function=embedding_function,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each user message we are going to use 3 LLM requests to synthesise the answer, namely for contextualization, query analysis and final generation.\n",
    "\n",
    "Throughout this notebook we're going to use the GPT-4o API via Azure.\n",
    "However, the first two steps arguably don't need to be performed by such a heavy model, and neither of them need to be performed by a model provided by OpenAI.\n",
    "The only requirement for the model is that it needs to have **function calling** capabilities.\n",
    "\n",
    "LangChain supports sevaral of such models, enabling us to use them as drop in replacements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a LangChain chat model instance\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    temperature=0.0,\n",
    "    azure_deployment=\"gpt4o\",\n",
    "    openai_api_version=\"2023-07-01-preview\",\n",
    "    max_retries=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a retriever that finds relevant documents via query analysis and vector search\n",
    "\n",
    "retriever = build_retriever(\n",
    "    llm=llm, vectorstore=index.vectorstore, docstore=index.docstore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a generator that responds to user questions in a conversation using documentation\n",
    "\n",
    "generator = build_generator(llm=llm, retriever=retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a response\n",
    "\n",
    "Here we use a basic LangChain `invoke` call to generate the answer based on a query. This can also be replaced by a `stream` call to receive a streaming response instead, to avoid having the user wait while the entire response gets generated.\n",
    "\n",
    "**Important**: exact contents of the `config` argument are going to vary depending on the way the chat history is handled by a particular application.\n",
    "\n",
    "To see an example of non-default chat history management and handling the streaming response, see `demo.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(False)  # see all under-the-hood operations performed by langchain\n",
    "\n",
    "# generate a response\n",
    "response = generator.invoke(\n",
    "    {\"input\": \"Can I use row-level security on EdgeDB?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"1\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unwrap the response for ease of reading\n",
    "\n",
    "def pretty_print_response(response):\n",
    "    print(f\"************PROMPT************\\n\\n{response['input']}\\n\\n\")\n",
    "    print(f\"************ANSWER************\\n\\n{response['answer'].answer}\\n\\n\")\n",
    "\n",
    "    print(f\"***********CITATIONS**********\\n\\n\")\n",
    "\n",
    "    for citation in response[\"answer\"].citations:\n",
    "        doc = response[\"retrieval_result\"][\"documents\"][citation.source_id]\n",
    "        print(f\"Source {citation.source_id}: {doc.metadata['source']}\\n\")\n",
    "        print(f\"Category: {doc.metadata['category']}\\n\")\n",
    "        print(f\"Quote:\\n{citation.quote}\\n\\n\")\n",
    "\n",
    "    print(f\"*********SEARCH TERMS*********\\n\")\n",
    "    search_terms = response[\"retrieval_result\"][\"search_terms\"]\n",
    "    print(f\"Query: {search_terms.query}\")\n",
    "    print(f\"Filter: {search_terms.category}\\n\\n\")\n",
    "\n",
    "    print(f\"************SOURCES***********\\n\\n\")\n",
    "\n",
    "    for i, doc in enumerate(response[\"retrieval_result\"][\"documents\"]):\n",
    "        print(f\"Source {i}: {doc.metadata['source']}\\n\")\n",
    "        print(f\"Category: {doc.metadata['category']}\\n\")\n",
    "        print(f\"Content:\\n{doc.page_content}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edgedb_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
