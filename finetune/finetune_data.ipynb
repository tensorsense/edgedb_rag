{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv(\"us.env\"))  # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "sys.path.append(Path(\"..\").resolve().as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the library\n",
    "\n",
    "...and split it by categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_path = Path(\"../../docs_md\").resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(lib_path.as_posix(), recursive=True).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core.index import DOC_MAP\n",
    "\n",
    "doc_map_keys = list(DOC_MAP.keys())\n",
    "\n",
    "docs_with_meta = []\n",
    "\n",
    "for doc in documents:\n",
    "    rel_path = str(\n",
    "        Path(doc.metadata[\"file_path\"]).resolve().relative_to(lib_path).as_posix()\n",
    "    )\n",
    "\n",
    "    section_key = [key for key in doc_map_keys if rel_path.startswith(key)][\n",
    "        -1\n",
    "    ]  # because cli and clients\n",
    "    doc.metadata[\"section\"] = DOC_MAP[section_key]\n",
    "    docs_with_meta.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare LangChain pipeline\n",
    "\n",
    "We're using a Pydantic structured output chain because we need the data to adhere a certain structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_community.chat_models import AzureChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from langchain_core.runnables import RunnableSerializable\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import Type, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessagePair(BaseModel):\n",
    "    human: str = Field(description=\"Message from the user.\")\n",
    "    assistant: str = Field(description=\"Helpful response from the assistant.\")\n",
    "\n",
    "\n",
    "class Conversation(BaseModel):\n",
    "    message_pairs: List[MessagePair] = Field(\n",
    "        description=\"A short series of back and forth messages between the human and the assistant.\"\n",
    "    )\n",
    "\n",
    "\n",
    "system_template = \"\"\"You are an assistant that helps write scripts of conversations about EdgeDB.\n",
    "    Below you will find a piece of official EdgeDB documentation denoted by ---.\n",
    "    Your job is to write a script of a conversation based on that piece of documentation.\n",
    "    There're two participants: a human who wants to learn about / needs help with EdgeDB, EdgeQL, SDL, DDL, clent integrations etc.\n",
    "    There's also a helpful assistant, whos job is two help the human out.\n",
    "\n",
    "    Please only use the provided piece of documentation and no prior knowledge to create the conversation.\n",
    "    Make sure some code snippets are involved.\n",
    "    When providing SDL examples, please replace old syntax with up to date syntax, in which \"->\" is used over \":\", and \"property\" and \"link\" are ommitted for non-computed properties/links.\n",
    "    Examples:\n",
    "    Old:\n",
    "    ```sdl\n",
    "    type Movie {{\n",
    "    required property title -> str;\n",
    "    multi link actors -> Person;\n",
    "    }}\n",
    "    ```\n",
    "    Up to date:\n",
    "    ``sdl\n",
    "    type Movie {{\n",
    "    required title: str;\n",
    "    multi actors: Person;\n",
    "    }}\n",
    "    ```\n",
    "\n",
    "    {format_instructions}\n",
    "    \"\"\"\n",
    "\n",
    "human_template = \"\"\"---{doc}---\n",
    "    \"\"\"\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    azure_deployment=\"gpt-4-1106\",\n",
    "    openai_api_version=\"2023-07-01-preview\",\n",
    ")\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=Conversation)\n",
    "\n",
    "system_message = SystemMessagePromptTemplate.from_template(\n",
    "    template=system_template,\n",
    "    # partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# system_message.partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "# print(system_message)\n",
    "\n",
    "human_message = HumanMessagePromptTemplate.from_template(template=human_template)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_message,\n",
    "        human_message,\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt = prompt.partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As a result of calling this chain for a piece of documentation, we're expecting to get a `Coversation` object.\n",
    "- In it, there's going to be a series of generated user questions and assistant answers wrapped as `MessagePair` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# For this example we only want docs that are related to EdgeQL and SDL\n",
    "\n",
    "docs_edgeql_sdl = []\n",
    "\n",
    "for doc in docs_with_meta:\n",
    "    if doc.metadata[\"section\"] == \"edgeql_and_sdl\":\n",
    "        docs_edgeql_sdl.append(doc)\n",
    "\n",
    "docs_edgeql_sdl = random.sample(docs_edgeql_sdl, 20)\n",
    "\n",
    "len(docs_edgeql_sdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = \"_val\"\n",
    "conversations_path = Path(f\"edgeql_sdl_conversations{suffix}.jsonl\")\n",
    "\n",
    "responses = []\n",
    "\n",
    "for doc in tqdm(docs_edgeql_sdl):\n",
    "    try:\n",
    "        response = chain.invoke({\"doc\": doc})\n",
    "    except:\n",
    "        continue\n",
    "    with conversations_path.open(\"a+\") as f:\n",
    "        f.write(response.json())\n",
    "        f.write(\"\\n\")\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a generated dialog\n",
    "\n",
    "for turn in responses[0].message_pairs:\n",
    "    print(f\"Human: \\n\\n{turn.human}\\n\")   \n",
    "    print(f\"Assistant: \\n\\n{turn.assistant}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repackage results\n",
    "\n",
    "We need them to fit OpenAI data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "\n",
    "class Message(BaseModel):\n",
    "    role: Literal[\"system\", \"user\", \"assistant\"]\n",
    "    content: str\n",
    "\n",
    "class Chat(BaseModel):\n",
    "    messages: List[Message]\n",
    "\n",
    "class Dataset(BaseModel):\n",
    "    chats: List[Chat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chats = []\n",
    "\n",
    "for response in responses:\n",
    "    messages = []\n",
    "    for turn in response.message_pairs:\n",
    "        messages.append(Message(role=\"user\", content=turn.human))\n",
    "        messages.append(Message(role=\"assistant\", content=turn.assistant))\n",
    "\n",
    "    chat = Chat(messages=messages)\n",
    "    chats.append(chat)\n",
    "\n",
    "dataset = Dataset(chats=chats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_path = Path(f\"edgeql_sdl_formatted_v1{suffix}.jsonl\")\n",
    "\n",
    "with formatted_path.open(\"w\") as f:\n",
    "    for chat in dataset.chats:\n",
    "        f.write(f\"{chat.json()}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
